---
title: An Implicit Physical Face Model Driven by Expression and Style"
collection: publications
permalink: /publication/2023e_physicsstyle
teaser: /images/physicsstyle2023.png
excerpt: We propose a new face model based on a data-driven implicit neural physics model that can be driven by both expression and style separately. At the core, we present a framework for learning implicit physics-based actuations for multiple subjects simultaneously, trained on a few arbitrary performance capture sequences from a small set of identities.  [[Project Page]](https://studios.disneyresearch.com/2023/11/29/an-implicit-physical-face-model-driven-by-expression-and-style/)<br>
date: 2023-11-23
venue: 'Siggraph Asia'
bibtex: "
@inproceedings{Yang2023,<br>
    author = {Yang, Lingchen and Zoss, Gaspard and Chandran, Prashanth and Gotardo, Paulo and Gross, Markus and Solenthaler, Barbara and Sifakis, Eftychios and Bradley, Derek},<br>
    title = {An Implicit Physical Face Model Driven by Expression and Style},<br>
    year = {2023},<br>
    isbn = {9798400703157},<br>
    publisher = {Association for Computing Machinery},<br>
    address = {New York, NY, USA},<br>
    doi = {10.1145/3610548.3618156},<br>
    booktitle = {SIGGRAPH Asia 2023 Conference Papers},<br>
    articleno = {106},<br>
    numpages = {12},<br>
    location = {, Sydney, NSW, Australia, },<br>
    series = {SA '23}<br>
}<br>
"
---

**Abstract**
<p>
3D facial animation is often produced by manipulating facial deformation models (or rigs) that are traditionally parameterized by expression controls. A key component that is usually overlooked is expression “style”, as in, how a particular expression is performed. Although it is common to define a semantic basis of expressions that characters can perform, most characters perform each expression in their own style. To date, style is usually entangled with the expression, and it is not possible to transfer the style of one character to another when considering facial animation. We present a new face model, based on a data-driven implicit neural physics model, that can be driven by both expression and style separately. At the core, we present a framework for learning implicit physics-based actuations for multiple subjects simultaneously, trained on a few arbitrary performance capture sequences from a small set of identities. Once trained, our method allows generalized physics-based facial animation for any of the trained identities, extending to unseen performances. Furthermore, it grants control over the animation style, enabling style transfer from one character to another or blending styles of different characters. Lastly, as a physics-based model, it is capable of synthesizing physical effects, such as collision handling, setting our method apart from conventional approaches.
</p>

[Project Page](https://studios.disneyresearch.com/2023/11/29/an-implicit-physical-face-model-driven-by-expression-and-style/)

**Bibtex:** 
<pre>
@inproceedings{Yang2023,
    author = {Yang, Lingchen and Zoss, Gaspard and Chandran, Prashanth and Gotardo, Paulo and Gross, Markus and Solenthaler, Barbara and Sifakis, Eftychios and Bradley, Derek},
    title = {An Implicit Physical Face Model Driven by Expression and Style},
    year = {2023},
    isbn = {9798400703157},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3610548.3618156},
    booktitle = {SIGGRAPH Asia 2023 Conference Papers},
    articleno = {106},
    numpages = {12},
    location = {, Sydney, NSW, Australia, },
    series = {SA '23}
}
</pre>
{: .notice}